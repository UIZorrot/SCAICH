from bs4 import BeautifulSoup
import concurrent
from flask import Flask, request, jsonify
from flask_caching import Cache
from flask_cors import CORS, cross_origin
from pymilvus import MilvusClient
import requests
import time
import requests_cache
from utils import fetch_openalex_papers, get_paper_info, fetch_openalex_papers_first
from fetch_arxiv import PaperSearcher
from scihub_search import (
    keyword_ollama,
    paragraph_to_json_dp,
    paragraph_to_json_ollama,
    search_scihub,
    trans_ollama,
)
from typing import List, Dict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import sqlite3
import concurrent.futures
from bs4 import BeautifulSoup
from utils import search_top_50


def extract_text_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–çº¯æ–‡æœ¬ï¼Œå»é™¤æ‰€æœ‰ HTML æ ‡ç­¾ã€‚

    :param html_content: è¾“å…¥çš„ HTML å­—ç¬¦ä¸²
    :return: æå–åçš„çº¯æ–‡æœ¬
    """
    # ä½¿ç”¨ BeautifulSoup è§£æ HTML å†…å®¹
    soup = BeautifulSoup(html_content, "html.parser")

    # è·å–å»é™¤æ ‡ç­¾åçš„çº¯æ–‡æœ¬
    return soup.get_text()


def sort_papers_by_relevance(papers: List[Dict], query: str) -> List[Dict]:
    # ä½¿ç”¨é›†åˆå»é‡æ ‡é¢˜
    seen_titles = set()
    unique_papers = []

    for paper in papers:
        # è¿‡æ»¤æ²¡æœ‰ doi æˆ– location ä¸ç¬¦åˆè¦æ±‚çš„è®ºæ–‡
        if paper.get("doi") is None or paper.get("location") == "Not Available":
            continue

        title = extract_text_from_html(paper["title"]).lower().replace(" ", "")
        if title not in seen_titles:
            unique_papers.append(paper)
            seen_titles.add(title)

    # å°† query å’Œæ‰€æœ‰ unique_paper çš„æ ‡é¢˜ç»„åˆåœ¨ä¸€èµ·
    documents = [query] + [f"{paper['title']}" for paper in unique_papers]

    # è®¡ç®— TF-IDF ç‰¹å¾
    vectorizer = TfidfVectorizer(
        stop_words="english", ngram_range=(1, 2)
    )  # å¢åŠ  ngram_range
    tfidf_matrix = vectorizer.fit_transform(documents)

    # è®¡ç®— query ä¸æ¯ä¸ª paper çš„ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_similarities = cosine_similarity(
        tfidf_matrix[0:1], tfidf_matrix[1:]
    ).flatten()

    # æŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„é¡¹ç›®
    fully_matched_papers = []
    non_fully_matched_papers = []

    # ä¸ºç›¸ä¼¼åº¦åˆ†é…æ•°å€¼
    similarity_map = {"highly related": 3, "related": 2, "barely related": 1}

    for i, paper in enumerate(unique_papers):
        if paper["title"].lower() == query.lower():  # åˆ¤æ–­æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            paper_with_similarity = paper.copy()
            paper_with_similarity["similarity"] = "fully matched"  # å®Œå…¨åŒ¹é…æ ‡è®°
            fully_matched_papers.append(paper_with_similarity)
        else:
            # å°†ç›¸ä¼¼åº¦å€¼åŠ å…¥åˆ° paper å­—å…¸ä¸­ï¼Œå¹¶ä¿ç•™ä¸¤ä½å°æ•°
            paper_with_similarity = paper.copy()
            similarity_value = round(cosine_similarities[i], 2)
            if similarity_value > 0.25:
                sim_tag = "highly related"
            elif similarity_value > 0.15:
                sim_tag = "related"
            else:
                sim_tag = "barely related"
            paper_with_similarity["similarity"] = sim_tag
            non_fully_matched_papers.append(paper_with_similarity)

    # å°†å®Œå…¨åŒ¹é…çš„è®ºæ–‡æ”¾åœ¨æœ€å‰é¢
    # å°†ç›¸ä¼¼åº¦æ ‡ç­¾æ˜ å°„ä¸ºæ•°å€¼è¿›è¡Œæ’åº
    sorted_non_fully_matched_papers = sorted(
        non_fully_matched_papers,
        key=lambda x: similarity_map[x["similarity"]],
        reverse=True,
    )

    # å°†å®Œå…¨åŒ¹é…çš„è®ºæ–‡å’Œéå®Œå…¨åŒ¹é…çš„è®ºæ–‡åˆå¹¶
    sorted_papers = fully_matched_papers + sorted_non_fully_matched_papers

    return sorted_papers


app = Flask(__name__)
CORS(app)  # å¯ç”¨ CORSï¼Œå…è®¸æ‰€æœ‰æ¥æºè®¿é—®
app.config["CORS_HEADERS"] = "Content-Type"
# é…ç½®ç¼“å­˜ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜
app.config["CACHE_TYPE"] = "SimpleCache"  # å¯ä»¥æ”¹ä¸º 'redis' æˆ– 'memcached' ç­‰
app.config["CACHE_DEFAULT_TIMEOUT"] = 3600  # é»˜è®¤ç¼“å­˜è¶…æ—¶ 5 åˆ†é’Ÿ
cache = Cache(app)

client = MilvusClient("http://localhost:19530")
collection_name = "scihub_rag"

@app.route("/searchfulltext", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
@cache.cached(timeout=60, query_string=True)  # ä½¿ç”¨ç¼“å­˜ï¼ŒæŒ‰æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºç¼“å­˜çš„å”¯ä¸€æ ‡è¯†
def searchfulltext():
    print()

@app.route("/search", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
@cache.cached(timeout=60, query_string=True)  # ä½¿ç”¨ç¼“å­˜ï¼ŒæŒ‰æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºç¼“å­˜çš„å”¯ä¸€æ ‡è¯†
def search():
    try:
        print(f"start: {time.time()}")
        # ä» URL å‚æ•°è·å– query å€¼
        query = request.args.get("query")
        limit = request.args.get("limit")
        oa = request.args.get("oa")
        if not query:
            return jsonify({"error": "Query parameter is required"}), 400

        if not limit or int(limit) <= 5:
            limit = 10
        else:
            limit = 20

        results = []

        query_org_lang = query
        try:
            search_doi = get_paper_info(query_org_lang)
            if search_doi["source"] and search_doi["source"] == "s":
                results.append(search_doi)
        except:
            print("no doi")

        print(f"finish 1: {time.time()}")

        query_org = trans_ollama(query)
        if query_org == "" or query_org.lower() == "none":
            query_org = query_org_lang
        print(query_org)
        query = keyword_ollama(query_org)
        if query == "" or query.lower() == "none":
            query = query_org_lang
        print(query)

        print(f"finish 2: {time.time()}")
        # ğŸ”¹ 3ï¸âƒ£ ç»§ç»­æŸ¥è¯¢ Milvus
        data = search_scihub(client, collection_name, query, limit)
        results.append(data)
        print(f"finish 3: {time.time()}")

        # å¹¶å‘è¯·æ±‚ OpenAlex
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            # å¹¶å‘æ‰§è¡Œ fetch_openalex_papers
            # futures_process = [executor.submit(process_item, item) for item in data]

            if query == query_org:
                future_arxiv_res = executor.submit(
                    fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
                )
                arxiv_res = future_arxiv_res.result()
                future_arxiv_res_org = []
                results.extend(arxiv_res)
            else:
                future_arxiv_res = executor.submit(
                    fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
                )
                future_arxiv_res_org = executor.submit(
                    fetch_openalex_papers, query_org, limit * 2, oa
                )
                arxiv_res = future_arxiv_res.result()
                arxiv_res_org = future_arxiv_res_org.result()
                results.extend(arxiv_res)
                results.extend(arxiv_res_org)

            # for future in concurrent.futures.as_completed(futures_process):
            #     results.append(future.result())

        # # ğŸ”¹ 0ï¸âƒ£ å…ˆå¼‚æ­¥æ‰§è¡Œ Arxiv æŸ¥æ‰¾
        # with ThreadPoolExecutor() as executor:
        #     # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç† Milvus æ•°æ®å’Œ CrossRef çš„ç¬¬äºŒæ¬¡æŸ¥æ‰¾
        #     futures = []
        #     for item in data:
        #         futures.append(executor.submit(process_item, item, crossref_set))

        #     # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        #     for future in as_completed(futures):
        #         result = future.result()
        #         if result:  # åªæœ‰åœ¨æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è¿½åŠ 
        #             results.append(result)

        print(f"finish 4: {time.time()}")

        res_bm25 = search_top_50(query)

        results.extend(res_bm25)
        # ğŸ”¹ 5ï¸âƒ£ å¯¹ç»“æœè¿›è¡Œæ’åº
        results = sort_papers_by_relevance(results, query_org)

        # å¦‚æœ DOI å·²åœ¨ SciHubï¼Œåˆ™ç›´æ¥è¿”å›

        sum = paragraph_to_json_dp(query_org_lang, str(results[:3]), limit)

        response = {
            "summary": sum,
            "results": results[: (limit * 2)],
        }

        print(f"finish 5: {time.time()}")

        return jsonify(response)

    except Exception as e:
        print(e)
        response = {
            "summary": {
                "cot": "",
                "sum": "Sorry, something wrong is happened, please try again",
            },
            "results": [],
        }
        return jsonify(response)


@app.route("/searchscihub", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
@cache.cached(timeout=60, query_string=True)  # ä½¿ç”¨ç¼“å­˜ï¼ŒæŒ‰æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºç¼“å­˜çš„å”¯ä¸€æ ‡è¯†
def searchscihub():
    try:
        print(f"start: {time.time()}")
        # ä» URL å‚æ•°è·å– query å€¼
        query = request.args.get("query")
        token = request.args.get("token")
        if token != "ao1ni1@*Njri1j*fi1iPPP11$5512xf1":
            return jsonify({"error": "Token is invalid"}), 400
        limit = 50
        oa = False
        if not query:
            return jsonify({"error": "Query parameter is required"}), 400

        query_org_lang = query

        print(f"finish 1: {time.time()}")

        results = []

        try:
            search_doi = get_paper_info(query_org_lang)
            if search_doi["source"] and search_doi["source"] == "s":
                results.append(search_doi)
        except:
            print("no doi")

        print(f"finish 2: {time.time()}")
        # ğŸ”¹ 3ï¸âƒ£ ç»§ç»­æŸ¥è¯¢ Milvus
        data = search_scihub(client, collection_name, query, limit)
        results.append(data)
        print(f"finish 3: {time.time()}")

        # å¹¶å‘è¯·æ±‚ OpenAlex
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            # å¹¶å‘æ‰§è¡Œ fetch_openalex_papers
            # futures_process = [executor.submit(process_item, item) for item in data]

            future_arxiv_res = executor.submit(
                fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
            )
            arxiv_res = future_arxiv_res.result()
            results.extend(arxiv_res)

            # for future in concurrent.futures.as_completed(futures_process):
            #     results.append(future.result())

        # # ğŸ”¹ 0ï¸âƒ£ å…ˆå¼‚æ­¥æ‰§è¡Œ Arxiv æŸ¥æ‰¾
        # with ThreadPoolExecutor() as executor:
        #     # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç† Milvus æ•°æ®å’Œ CrossRef çš„ç¬¬äºŒæ¬¡æŸ¥æ‰¾
        #     futures = []
        #     for item in data:
        #         futures.append(executor.submit(process_item, item, crossref_set))

        #     # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        #     for future in as_completed(futures):
        #         result = future.result()
        #         if result:  # åªæœ‰åœ¨æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è¿½åŠ 
        #             results.append(result)

        print(f"finish 4: {time.time()}")

        res_bm25 = search_top_50(query)

        results.extend(res_bm25)

        # ğŸ”¹ 5ï¸âƒ£ å¯¹ç»“æœè¿›è¡Œæ’åº
        results = sort_papers_by_relevance(results, query)

        res = []
        # å¦‚æœ DOI å·²åœ¨ SciHubï¼Œåˆ™ç›´æ¥è¿”å›
        for paper in results:
            if paper["source"] == "s":
                res.append(paper)

        # sum = paragraph_to_json_dp(query_org_lang, str(results[:3]), limit)

        response = {
            # "summary": sum,
            "results": res,
        }

        print(f"finish 5: {time.time()}")

        return jsonify(response)

    except Exception as e:
        print(e)
        response = {
            "summary": {
                "cot": "",
                "sum": "Sorry, something wrong is happened, please try again",
            },
            "results": [],
        }
        return jsonify(response)


# å¤„ç†å•ä¸ª item çš„å‡½æ•°
def process_item(item):
    if "title" not in item or "doi" not in item:
        return None

    doi = item["doi"]

    # **æŸ¥è¯¢ CrossRef è·å–è¯¦ç»†ä¿¡æ¯**
    paper_info = get_paper_info(doi)

    return paper_info


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=7788)
