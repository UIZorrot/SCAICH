import os
from bs4 import BeautifulSoup
import concurrent
from flask import Flask, request, jsonify
from flask_caching import Cache
from flask_cors import CORS, cross_origin
from flask_socketio import SocketIO, emit
from pymilvus import MilvusClient
import requests
import time
import requests_cache
import socketio
from fetch_arxiv_pdf import get_arxiv
from fetch_scihub import get_scihub
from rag import chat_rag_init, query_rag, chat_init
from utils import fetch_openalex_papers, get_paper_info, fetch_openalex_papers_first
from fetch_arxiv import PaperSearcher
from scihub_search import (
    keyword_ollama,
    paragraph_to_json_dp,
    paragraph_to_json_ollama,
    search_scihub,
    trans_ollama,
)
from typing import List, Dict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import sqlite3
import concurrent.futures
from bs4 import BeautifulSoup
from utils import search_top_50
import hashlib
import math
import uuid
from datetime import datetime

def extract_text_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–çº¯æ–‡æœ¬ï¼Œå»é™¤æ‰€æœ‰ HTML æ ‡ç­¾ã€‚

    :param html_content: è¾“å…¥çš„ HTML å­—ç¬¦ä¸²
    :return: æå–åçš„çº¯æ–‡æœ¬
    """
    # ä½¿ç”¨ BeautifulSoup è§£æ HTML å†…å®¹
    soup = BeautifulSoup(html_content, "html.parser")

    # è·å–å»é™¤æ ‡ç­¾åçš„çº¯æ–‡æœ¬
    return soup.get_text()


def sort_papers_by_relevance(papers: List[Dict], query: str) -> List[Dict]:
    # ä½¿ç”¨é›†åˆå»é‡æ ‡é¢˜
    seen_titles = set()
    unique_papers = []

    for paper in papers:
        # è¿‡æ»¤æ²¡æœ‰ doi æˆ– location ä¸ç¬¦åˆè¦æ±‚çš„è®ºæ–‡
        # or paper.get("location") == "Not Available"
        # print(paper)
        if paper == "error" or paper.get("doi") is None or paper.get("title") is None:
            continue

        title = extract_text_from_html(paper["title"]).lower().replace(" ", "")
        if title not in seen_titles:
            unique_papers.append(paper)
            seen_titles.add(title)

    # print("hrer2")

    # å°† query å’Œæ‰€æœ‰ unique_paper çš„æ ‡é¢˜ç»„åˆåœ¨ä¸€èµ·
    documents = [query] + [f"{paper['title']}" for paper in unique_papers]

    # è®¡ç®— TF-IDF ç‰¹å¾
    vectorizer = TfidfVectorizer(
        stop_words="english", ngram_range=(1, 2)
    )  # å¢åŠ  ngram_range
    tfidf_matrix = vectorizer.fit_transform(documents)

    # è®¡ç®— query ä¸æ¯ä¸ª paper çš„ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_similarities = cosine_similarity(
        tfidf_matrix[0:1], tfidf_matrix[1:]
    ).flatten()

    # æŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„é¡¹ç›®
    fully_matched_papers = []
    non_fully_matched_papers = []

    # ä¸ºç›¸ä¼¼åº¦åˆ†é…æ•°å€¼
    similarity_map = {"highly related": 3, "related": 2, "barely related": 1}

    for i, paper in enumerate(unique_papers):
        if paper["title"].lower() == query.lower():  # åˆ¤æ–­æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            paper_with_similarity = paper.copy()
            paper_with_similarity["similarity"] = "fully matched"  # å®Œå…¨åŒ¹é…æ ‡è®°
            fully_matched_papers.append(paper_with_similarity)
        else:
            # å°†ç›¸ä¼¼åº¦å€¼åŠ å…¥åˆ° paper å­—å…¸ä¸­ï¼Œå¹¶ä¿ç•™ä¸¤ä½å°æ•°
            paper_with_similarity = paper.copy()
            similarity_value = round(cosine_similarities[i], 2)
            if similarity_value > 0.25:
                sim_tag = "highly related"
            elif similarity_value > 0.15:
                sim_tag = "related"
            else:
                sim_tag = "barely related"
            paper_with_similarity["similarity"] = sim_tag
            non_fully_matched_papers.append(paper_with_similarity)

    # å°†å®Œå…¨åŒ¹é…çš„è®ºæ–‡æ”¾åœ¨æœ€å‰é¢
    # å°†ç›¸ä¼¼åº¦æ ‡ç­¾æ˜ å°„ä¸ºæ•°å€¼è¿›è¡Œæ’åº
    sorted_non_fully_matched_papers = sorted(
        non_fully_matched_papers,
        key=lambda x: similarity_map[x["similarity"]],
        reverse=True,
    )

    # å°†å®Œå…¨åŒ¹é…çš„è®ºæ–‡å’Œéå®Œå…¨åŒ¹é…çš„è®ºæ–‡åˆå¹¶
    sorted_papers = fully_matched_papers + sorted_non_fully_matched_papers

    return sorted_papers


app = Flask(__name__)
CORS(app)  # å¯ç”¨ CORSï¼Œå…è®¸æ‰€æœ‰æ¥æºè®¿é—®
app.config["CORS_HEADERS"] = "Content-Type"
# é…ç½®ç¼“å­˜ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜
app.config["CACHE_TYPE"] = "SimpleCache"  # å¯ä»¥æ”¹ä¸º 'redis' æˆ– 'memcached' ç­‰
app.config["CACHE_DEFAULT_TIMEOUT"] = 3600  # é»˜è®¤ç¼“å­˜è¶…æ—¶ 5 åˆ†é’Ÿ
cache = Cache(app)

# client = MilvusClient("http://localhost:19530")
# collection_name = "scihub_rag"

@app.route("/search", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
@cache.cached(timeout=60, query_string=True)  # ä½¿ç”¨ç¼“å­˜ï¼ŒæŒ‰æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºç¼“å­˜çš„å”¯ä¸€æ ‡è¯†
def search():
    try:
        # print(f"start: {time.time()}")
        # ä» URL å‚æ•°è·å– query å€¼
        query = request.args.get("query")
        limit = request.args.get("limit")
        oa = request.args.get("oa")
        ai = request.args.get("ai")
        
        if not ai:
            ai = True
        else:
            if ai.lower() == "false":
                ai = False
            else:
                ai = True


        print(ai)

        if not query:
            return jsonify({"error": "Query parameter is required"}), 400

        limit = 20

        results = []

        # print("no new problem")

        query_org_lang = query
        doi_flag = False
        try:
            search_doi = get_paper_info(query_org_lang)
            if search_doi["source"]:
                results.append(search_doi)
                query_org_lang = f"Find the paper with {query_org_lang}"
                doi_flag = True
        except:
            print("no doi")

        # print(f"finish 1: {time.time()}")

        if doi_flag == False:
            query_org = trans_ollama(query)
            if query_org == "" or query_org.lower() == "none":
                query_org = query_org_lang
            print(query_org)
            query = keyword_ollama(query_org)
            if query == "" or query.lower() == "none":
                query = query_org_lang
            print(query)

            # print(f"finish 2: {time.time()}")
            # ğŸ”¹ 3ï¸âƒ£ ç»§ç»­æŸ¥è¯¢ Milvus
            # data = search_scihub(client, collection_name, query, limit)
            # print(f"finish 3: {time.time()}")

            # å¹¶å‘è¯·æ±‚ OpenAlex
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                # å¹¶å‘æ‰§è¡Œ fetch_openalex_papers
                # futures_process = [executor.submit(process_item, item) for item in data]

                if query == query_org:
                    future_arxiv_res = executor.submit(
                        fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
                    )
                    arxiv_res = future_arxiv_res.result()
                    future_arxiv_res_org = []
                    results.extend(arxiv_res)
                else:
                    future_arxiv_res = executor.submit(
                        fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
                    )
                    future_arxiv_res_org = executor.submit(
                        fetch_openalex_papers, query_org, limit * 2, oa
                    )
                    arxiv_res = future_arxiv_res.result()
                    arxiv_res_org = future_arxiv_res_org.result()
                    results.extend(arxiv_res)
                    results.extend(arxiv_res_org)

                # for future in concurrent.futures.as_completed(futures_process):
                #     results.append(future.result())

            # # ğŸ”¹ 0ï¸âƒ£ å…ˆå¼‚æ­¥æ‰§è¡Œ Arxiv æŸ¥æ‰¾
            # with ThreadPoolExecutor() as executor:
            #     # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç† Milvus æ•°æ®å’Œ CrossRef çš„ç¬¬äºŒæ¬¡æŸ¥æ‰¾
            #     futures = []
            #     for item in data:
            #         futures.append(executor.submit(process_item, item, crossref_set))

            #     # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
            #     for future in as_completed(futures):
            #         result = future.result()
            #         if result:  # åªæœ‰åœ¨æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è¿½åŠ 
            #             results.append(result)

            # print(results)

            print(f"finish 4: {time.time()}")

            res_bm25 = search_top_50(query)

            results.extend(res_bm25)

            # print(results)
            
            # ğŸ”¹ 5ï¸âƒ£ å¯¹ç»“æœè¿›è¡Œæ’åº
            print(f"finish 5: {time.time()}")

            # print(results)
            # print(query_org)
            results = sort_papers_by_relevance(results, query_org)

        # å¦‚æœ DOI å·²åœ¨ SciHubï¼Œåˆ™ç›´æ¥è¿”å›
        print(f"finish 6: {time.time()}")    
        if ai:
            sum = paragraph_to_json_dp(query_org_lang, str(results[:6]), limit)
        else:
            sum = {
                "cot": "",
                "sum": "User require no AI respone",
            }

        response = {
            "summary": sum,
            "results": results,
        }

        # print(f"finish 7: {time.time()}")

        return jsonify(response)

    except Exception as e:
        print(e)
        response = {
            "summary": {
                "cot": "",
                "sum": "Sorry, something wrong is happened, please try again",
            },
            "results": [],
        }
        return jsonify(response)

@app.route("/scihub", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
@cache.cached(timeout=60, query_string=True)  # ä½¿ç”¨ç¼“å­˜ï¼ŒæŒ‰æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºç¼“å­˜çš„å”¯ä¸€æ ‡è¯†
def scihub():
    try:
        # print(f"start: {time.time()}")
        # ä» URL å‚æ•°è·å– query å€¼
        query = request.args.get("query")
        limit = request.args.get("limit")
        oa = True

        if not query:
            return jsonify({"error": "Query parameter is required"}), 400
        results = []

        # print("no new problem")

        query_org_lang = query
        doi_flag = False
        try:
            search_doi = get_paper_info(query_org_lang)
            if search_doi["source"]:
                results.append(search_doi)
                query_org_lang = f"Find the paper with {query_org_lang}"
                doi_flag = True
        except:
            print("no doi")

        # print(f"finish 1: {time.time()}")

        if doi_flag == False:
            # print(query_org)
            query = keyword_ollama(query)
            if query == "" or query.lower() == "none":
                query = query_org_lang
            # print(query)

            # print(f"finish 2: {time.time()}")
            # ğŸ”¹ 3ï¸âƒ£ ç»§ç»­æŸ¥è¯¢ Milvus
            # data = search_scihub(client, collection_name, query, limit)
            # print(f"finish 3: {time.time()}")

            # å¹¶å‘è¯·æ±‚ OpenAlex
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                # å¹¶å‘æ‰§è¡Œ fetch_openalex_papers
                # futures_process = [executor.submit(process_item, item) for item in data]

                if query == query_org_lang:
                    future_arxiv_res = executor.submit(
                        fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
                    )
                    arxiv_res = future_arxiv_res.result()
                    future_arxiv_res_org = []
                    results.extend(arxiv_res)
                else:
                    future_arxiv_res = executor.submit(
                        fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
                    )
                    future_arxiv_res_org = executor.submit(
                        fetch_openalex_papers, query_org_lang, limit * 2, oa
                    )
                    arxiv_res = future_arxiv_res.result()
                    arxiv_res_org = future_arxiv_res_org.result()
                    results.extend(arxiv_res)
                    results.extend(arxiv_res_org)

                # for future in concurrent.futures.as_completed(futures_process):
                #     results.append(future.result())

            # # ğŸ”¹ 0ï¸âƒ£ å…ˆå¼‚æ­¥æ‰§è¡Œ Arxiv æŸ¥æ‰¾
            # with ThreadPoolExecutor() as executor:
            #     # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç† Milvus æ•°æ®å’Œ CrossRef çš„ç¬¬äºŒæ¬¡æŸ¥æ‰¾
            #     futures = []
            #     for item in data:
            #         futures.append(executor.submit(process_item, item, crossref_set))

            #     # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
            #     for future in as_completed(futures):
            #         result = future.result()
            #         if result:  # åªæœ‰åœ¨æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è¿½åŠ 
            #             results.append(result)

            # print(f"finish 4: {time.time()}")

            res_bm25 = search_top_50(query)

            results.extend(res_bm25)
            # ğŸ”¹ 5ï¸âƒ£ å¯¹ç»“æœè¿›è¡Œæ’åº
            # print(f"finish 5: {time.time()}")

            print(results)
            # print(query_org)
            results = sort_papers_by_relevance(results, query_org_lang)

        # å¦‚æœ DOI å·²åœ¨ SciHubï¼Œåˆ™ç›´æ¥è¿”å›
        # print(f"finish 6: {time.time()}")    

        sum = {
            "cot": "",
            "sum": "User require no AI respone",
        }

        response = {
            "summary": sum,
            "results": results,
        }

        # print(f"finish 7: {time.time()}")

        return jsonify(response)

    except Exception as e:
        # print(e)
        response = {
            "summary": {
                "cot": "",
                "sum": "Sorry, something wrong is happened, please try again",
            },
            "results": [],
        }
        return jsonify(response)


@app.route("/searchscihub", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
@cache.cached(timeout=60, query_string=True)  # ä½¿ç”¨ç¼“å­˜ï¼ŒæŒ‰æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºç¼“å­˜çš„å”¯ä¸€æ ‡è¯†
def searchscihub():
    try:
        # print(f"start: {time.time()}")
        # ä» URL å‚æ•°è·å– query å€¼
        query = request.args.get("query")
        token = request.args.get("token")
        if token != "ao1ni1@*Njri1j*fi1iPPP11$5512xf1":
            return jsonify({"error": "Token is invalid"}), 400
        limit = 50
        oa = False
        if not query:
            return jsonify({"error": "Query parameter is required"}), 400

        query_org_lang = query

        # print(f"finish 1: {time.time()}")

        results = []

        try:
            search_doi = get_paper_info(query_org_lang)
            if search_doi["source"] and search_doi["source"] == "s":
                results.append(search_doi)
        except:
            print("no doi")

        # print(f"finish 2: {time.time()}")
        # ğŸ”¹ 3ï¸âƒ£ ç»§ç»­æŸ¥è¯¢ Milvus
        # data = search_scihub(client, collection_name, query, limit)
        # print(f"finish 3: {time.time()}")

        # å¹¶å‘è¯·æ±‚ OpenAlex
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            # å¹¶å‘æ‰§è¡Œ fetch_openalex_papers
            # futures_process = [executor.submit(process_item, item) for item in data]

            future_arxiv_res = executor.submit(
                fetch_openalex_papers, query.replace(",", " "), limit * 2, oa
            )
            arxiv_res = future_arxiv_res.result()
            results.extend(arxiv_res)

            # for future in concurrent.futures.as_completed(futures_process):
            #     results.append(future.result())

        # # ğŸ”¹ 0ï¸âƒ£ å…ˆå¼‚æ­¥æ‰§è¡Œ Arxiv æŸ¥æ‰¾
        # with ThreadPoolExecutor() as executor:
        #     # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç† Milvus æ•°æ®å’Œ CrossRef çš„ç¬¬äºŒæ¬¡æŸ¥æ‰¾
        #     futures = []
        #     for item in data:
        #         futures.append(executor.submit(process_item, item, crossref_set))

        #     # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        #     for future in as_completed(futures):
        #         result = future.result()
        #         if result:  # åªæœ‰åœ¨æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è¿½åŠ 
        #             results.append(result)

        # print(f"finish 4: {time.time()}")

        res_bm25 = search_top_50(query)

        results.extend(res_bm25)

        # ğŸ”¹ 5ï¸âƒ£ å¯¹ç»“æœè¿›è¡Œæ’åº
        results = sort_papers_by_relevance(results, query)

        res = []
        # å¦‚æœ DOI å·²åœ¨ SciHubï¼Œåˆ™ç›´æ¥è¿”å›
        for paper in results:
            if paper["source"] == "s":
                res.append(paper)

        # sum = paragraph_to_json_dp(query_org_lang, str(results[:3]), limit)

        response = {
            # "summary": sum,
            "results": res,
        }

        # print(f"finish 5: {time.time()}")

        return jsonify(response)

    except Exception as e:
        # print(e)
        response = {
            "summary": {
                "cot": "",
                "sum": "Sorry, something wrong is happened, please try again",
            },
            "results": [],
        }
        return jsonify(response)

# å¤„ç†å•ä¸ª item çš„å‡½æ•°
def process_item(item):
    if "title" not in item or "doi" not in item:
        return None

    doi = item["doi"]

    # **æŸ¥è¯¢ CrossRef è·å–è¯¦ç»†ä¿¡æ¯**
    paper_info = get_paper_info(doi)

    return paper_info

# ---------------

chat_engines = {}
socketio = SocketIO(app, cors_allowed_origins="*")  # å…è®¸è·¨åŸŸï¼Œä¾¿äºå‰ç«¯æµ‹è¯•

p = """
You are an advanced RAG Agent with access to a knowledge base containing various academic papers. Your task is to analyze and summarize the content of all papers in your knowledge base in a highly detailed, structured, and readable way using Markdown format. Please follow these instructions:

1. Retrieve and analyze all papers in your knowledge base.
2. For each paper, provide the following:
   - Title
   - A detailed analysis of the paperâ€™s content, broken down into:
     - **Objective**: What problem or question the paper aims to address (1-2 sentences).
     - **Methodology**: How the research was conducted (e.g., techniques, datasets, experiments) (2-3 sentences).
     - **Key Findings**: The most important results or contributions (2-3 sentences).
     - **Implications**: The significance or potential impact of the findings (1-2 sentences).
3. Present the analysis in Markdown format with the following structure:
   - Use `##` for each paperâ€™s title
   - Use a bullet list (`-`) for the detailed analysis sections (Objective, Methodology, Key Findings, Implications), with each section labeled in bold (e.g., **Objective:**)
4. Sort the papers alphabetically by title.
5. If any information is missing, note it as "Not available" in the respective field.

Output the result as a complete Markdown document. Ensure the analysis is concise, logically structured, and highlights the core content of each paper.
"""

@socketio.on("connect")
def handle_connect():
    session_id = request.sid
    paper_id = request.args.get("id")  # å‰ç«¯ä¼ é€’çš„ doi
    source = request.args.get("source")  # å‰ç«¯ä¼ é€’çš„ sourceï¼ˆarxiv æˆ– scihubï¼‰
    dir_doc = ""

    if not paper_id or not source:
        emit("error", {"message": "Paper ID and source are required"})
        return

    if source == "scihub":
        dir_doc = get_scihub(paper_id)
    elif source == "arxiv":
        dir_doc = get_arxiv(paper_id)
    else:
        emit("error", {"message": "Invalid source"})
        return

    # print(dir_doc)
    chat_engine = chat_rag_init(p, dir_doc)
    if chat_engine is None:
        emit("error", {"message": "Failed to initialize chat engine"})
        return

    # ä¿å­˜ chat_engine åˆ°å…¨å±€å­—å…¸
    chat_engines[session_id] = chat_engine
    # print(
    #     f"Session {session_id} connected, chat_engine initialized for paper {paper_id} from {source}"
    # )
    emit("session_id", {"session_id": session_id})


# WebSocket äº‹ä»¶ï¼šå®¢æˆ·ç«¯æ–­å¼€æ—¶
@socketio.on("disconnect")
def handle_disconnect():
    session_id = request.sid
    if session_id in chat_engines:
        del chat_engines[session_id]
        print(f"Session {session_id} ended and memory released")
    else:
        print(f"Session {session_id} disconnected, but no chat_engine found")


# WebSocket äº‹ä»¶ï¼šå¤„ç†èŠå¤©è¯·æ±‚
@socketio.on("chat")
def handle_chat(data):
    session_id = request.sid
    query = data.get("query")
    if not query:
        emit("error", {"message": "Query is required"})
        return

    if session_id not in chat_engines:
        emit("error", {"message": "Session not found"})
        return

    chat_engine = chat_engines[session_id]
    try:
        response = chat_engine.stream_chat(query)
        for chunk in response.response_gen:
            emit("response", {"chunk": chunk})
            # print(f"Sent chunk to {session_id}: {chunk}")
    except Exception as e:
        emit("error", {"message": f"Error during query: {str(e)}"})
        # print(f"Error in session {session_id}: {str(e)}")


# Flask è·¯ç”±ï¼šå•æ¬¡æŸ¥è¯¢ï¼ˆä¿ç•™ HTTP æ¥å£ï¼‰
@app.route("/query", methods=["POST"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
def query():
    data = request.get_json()
    query_str = data.get("query")
    doc_dir = data.get("doc_dir", "default_doc_dir")
    index_dir = data.get("index_dir", "default_index_dir")
    temp_or_persist = data.get("temp_or_persist", "TEMP")

    if not query_str:
        return {"error": "Query is required"}, 400

    # å‡è®¾çš„ query_rag å‡½æ•°
    result = query_rag(query_str, doc_dir, index_dir, temp_or_persist)
    return {"response": result}

# ------------------------------  

def get_db():
    conn = sqlite3.connect("SCAIENGINE/backend/invites.db")
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    with get_db() as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS invites (
                invite_code TEXT PRIMARY KEY,
                user_id TEXT,
                created_at TEXT,
                used BOOLEAN NOT NULL
            )
        """)
        conn.commit()

# init_db()

@app.route("/invite", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
def verify_invite():
    invite_code = request.args.get("code")
    if not invite_code:
        return jsonify({"success": False, "message": "Invite code required"}), 400

    with get_db() as conn:
        cursor = conn.execute("SELECT * FROM invites WHERE invite_code = ?", (invite_code,))
        invite = cursor.fetchone()

        if not invite:
            return jsonify({"success": False, "message": "Invalid invite code"}), 400

        if invite["used"]:
            return jsonify({"success": False, "message": "Invite code already used"}), 400

        user_id = str(uuid.uuid4())
        conn.execute(
            "UPDATE invites SET user_id = ?, created_at = ?, used = ? WHERE invite_code = ?",
            (user_id, datetime.now().isoformat(), True, invite_code)
        )
        conn.commit()

        return jsonify({"success": True, "user_id": user_id})

@app.route("/verify-user", methods=["GET"])
@cross_origin(origin="*", headers=["Content-Type", "Authorization"])
def verify_user():
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "message": "User ID required"}), 400

    with get_db() as conn:
        cursor = conn.execute("SELECT * FROM invites WHERE user_id = ? AND used = ?", (user_id, True))
        user = cursor.fetchone()

        if not user:
            return jsonify({"success": False, "message": "Invalid user ID"}), 404

        return jsonify({"success": True, "user_id": user_id})
    
# -----------------------------

INVITES_DB = "SCAIENGINE/backend/invites.db"
KOL_DB = "SCAIENGINE/backend/kol_database.db"

def get_invites_db():
    """è¿æ¥åˆ° invites.db"""
    conn = sqlite3.connect(INVITES_DB)
    conn.row_factory = sqlite3.Row
    return conn

def get_kol_db():
    """è¿æ¥åˆ° kol_database.db"""
    conn = sqlite3.connect(KOL_DB)
    conn.row_factory = sqlite3.Row
    return conn


def generate_user_id():
    """ç”Ÿæˆå”¯ä¸€çš„ user_id"""
    return str(uuid.uuid4())


def generate_invite_code(user_id):
    """ç”Ÿæˆ invite_codeï¼ˆåŸºäº user_id çš„å“ˆå¸Œï¼‰"""
    return hashlib.sha256(user_id.encode()).hexdigest()[:16]


@app.route("/kolspec", methods=["POST"])
def redeem_kol_code():
    # è·å–å®¢æˆ·ç«¯ IP
    client_ip = request.headers.get('X-Forwarded-For', request.remote_addr)
    # å¦‚æœ X-Forwarded-For åŒ…å«å¤šä¸ª IPï¼ˆä»£ç†é“¾ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ª
    if ',' in client_ip:
        client_ip = client_ip.split(',')[0].strip()

    data = request.get_json()
    if not data or "code" not in data:
        return jsonify({"success": False, "message": "KOL code is required"}), 400

    code = data["code"]

    # è¿æ¥åˆ° kol_database.db
    kol_conn = get_kol_db()
    kol_cursor = kol_conn.cursor()

    # print(client_ip)
    # æ£€æŸ¥è¯¥IPæ˜¯å¦å·²ç»å…‘æ¢è¿‡
    kol_cursor.execute("SELECT user_id FROM redeemed_users WHERE ip_address = ?", (client_ip,))
    if kol_cursor.fetchone():
        kol_conn.close()
        return jsonify({"success": False, "message": "This IP has already redeemed a code"}), 403

    # æ£€æŸ¥ KOL ç æ˜¯å¦æœ‰æ•ˆ
    kol_cursor.execute("SELECT remaining_count FROM kols WHERE code = ?", (code,))
    kol_result = kol_cursor.fetchone()

    if not kol_result:
        kol_conn.close()
        return jsonify({"success": False, "message": "Invalid KOL code"}), 404

    remaining_count = kol_result["remaining_count"]

    if remaining_count <= 0:
        kol_conn.close()
        return (
            jsonify(
                {"success": False, "message": "No remaining user IDs for this KOL code"}
            ),
            400,
        )

    # ç”Ÿæˆ user_id å’Œ invite_code
    user_id = generate_user_id()
    invite_code = generate_invite_code(user_id)

    # å†™å…¥ invites.db å’Œ kol_database.db
    try:
        # å†™å…¥ invites.dbï¼ˆæœªä½¿ç”¨çŠ¶æ€ï¼‰
        invites_conn = get_invites_db()
        invites_conn.execute(
            "INSERT INTO invites (invite_code, user_id, created_at, used) VALUES (?, ?, ?, ?)",
            (invite_code, user_id, datetime.now().isoformat(), True),
        )
        invites_conn.commit()
        invites_conn.close()

        # æ›´æ–° kol_database.db
        kol_cursor.execute(
        "UPDATE kols SET remaining_count = remaining_count - 1 WHERE code = ?",
            (code,),
        )
        kol_cursor.execute(
            "INSERT INTO redeemed_users (code, user_id, invite_code, ip_address) VALUES (?, ?, ?, ?)",
            (code, user_id, invite_code, client_ip),
        )
        kol_conn.commit()
        kol_conn.close()

        return jsonify({"success": True, "user_id": user_id})
    except Exception as e:
        kol_conn.rollback()
        invites_conn = get_invites_db()
        invites_conn.rollback()
        invites_conn.close()
        kol_conn.close()
        return (
            jsonify({"success": False, "message": f"Error redeeming code: {str(e)}"}),
            500,
        )
    
# ----------------------

from flask import g

SCIHUB_DB = "scihub_dois.db"
ITEMS_PER_PAGE = 100

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼Œå¤ç”¨å·²å­˜åœ¨çš„è¿æ¥"""
    if 'db' not in g:
        g.db = sqlite3.connect(SCIHUB_DB)
        g.db.row_factory = sqlite3.Row
    return g.db

# @app.teardown_appcontext
# def close_db_connection(exception):
#     """åœ¨è¯·æ±‚ç»“æŸåå…³é—­æ•°æ®åº“è¿æ¥"""
#     db = g.pop('db', None)
#     if db is not None:
#         db.close()

def get_total_count():
    """è·å– DOI æ€»è®°å½•æ•°å¹¶ç¼“å­˜"""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM dois")
    total_count = cursor.fetchone()[0]
    return total_count

@app.route('/dois', methods=['GET'])
def get_dois():
    """åˆ†é¡µè·å– DOI æ•°æ®"""
    try:
        page = request.args.get('page', default=1, type=int)
        if page < 1:
            return jsonify({"error": "Page number must be greater than 0"}), 400

        offset = (page - 1) * ITEMS_PER_PAGE

        conn = get_db_connection()
        cursor = conn.cursor()

        # æŸ¥è¯¢å½“å‰é¡µæ•°æ®
        cursor.execute(
            "SELECT doi FROM dois LIMIT ? OFFSET ?",
            (ITEMS_PER_PAGE, offset)
        )
        dois = [row['doi'] for row in cursor.fetchall()]

        response = {
            "page": page,
            "per_page": ITEMS_PER_PAGE,
            "dois": dois
        }

        return jsonify(response), 200

    except sqlite3.Error as e:
        return jsonify({"error": f"Database error: {str(e)}"}), 500
    except Exception as e:
        return jsonify({"error": f"Server error: {str(e)}"}), 500
        
# ---------------

if __name__ == "__main__":
    # app.run(host="0.0.0.0", port=7788)
    socketio.run(app, host="0.0.0.0", port=7788)
